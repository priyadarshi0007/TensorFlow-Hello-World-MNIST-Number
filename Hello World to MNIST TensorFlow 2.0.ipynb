{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data \n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional neural networks (CNNs). \n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). \n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset mnist/3.0.0 (download: 11.06 MiB, generated: Unknown size, total: 11.06 MiB) to C:\\Users\\PSK\\tensorflow_datasets\\mnist\\3.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
      "local data directory. If you'd instead prefer to read directly from our public\n",
      "GCS bucket (recommended if you're running on GCP), you can instead set\n",
      "data_dir=gs://tfds-data/datasets.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b043609b002f48529101f24914c9de74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Dl Completed...', max=4, style=ProgressStyle(description_widtâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dataset mnist downloaded and prepared to C:\\Users\\PSK\\tensorflow_datasets\\mnist\\3.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# with_info=True will also provide us with a tuple containing information about the version, features, number of samples\n",
    "# we will use this information a bit below and we will store it in mnist_info\n",
    "# as_supervised=True will load the dataset in a 2-tuple structure (input, target) \n",
    "# alternatively, as_supervised=False, would return a dictionary\n",
    "\n",
    "mnist_dataset,mnist_info = tfds.load(name = 'mnist',with_info = True, as_supervised = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into train and test\n",
    "mnist_train,mnist_test = mnist_dataset['train'],mnist_dataset['test']\n",
    "\n",
    "#we need to get the validation set from the train set \n",
    "#we will be taking out 10% of the train dataset as validation set\n",
    "num_vaidation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "#we are casting it into numerical\n",
    "num_vaidation_samples = tf.cast(num_vaidation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples,tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we would like to scale our data in some way to make the result more numerically stable\n",
    "#in this case we will simply prefer to have inputs between 0 and 1\n",
    "def scale(image,label):\n",
    "    image = tf.cast(image,tf.float32)\n",
    "    # we are transforming the different shades of black from 0-255 into 0 and 1\n",
    "    image = image/255.\n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the method .map() allows us to apply a custom transformation to a given dataset\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "validation_data = shuffled_train_and_validation_data.take(num_vaidation_samples)\n",
    "#we skipped the validation data\n",
    "train_data = shuffled_train_and_validation_data.skip(num_vaidation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniBatch Gradient Descent\n",
    "\n",
    "Batch Size = 1 means Stochaistics Gradient Descent\n",
    "\n",
    "Batch Size = numbe of samples is normal Gradient Descent\n",
    "\n",
    "1 < Batch Size < Number of Sample = Mini Batch Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "#we will not be batching the test and validation dataset because in batching we calculate the average loss\n",
    "#we will be requiring accurate loos in the test and validation \n",
    "#one more reason is we will not be backpropagating in the test and validation data\n",
    "\n",
    "#here the total validation data is one batch \n",
    "validation_data = validation_data.batch(num_vaidation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "# takes next batch (it is the only batch)\n",
    "# because as_supervized=True, we've got a 2-tuple structure\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape =(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer,activation = 'relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer,activation = 'relu'),\n",
    "                            #output layer\n",
    "                            tf.keras.layers.Dense(output_size,activation = 'softmax')\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 6s - loss: 0.0464 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "540/540 - 5s - loss: 0.0409 - accuracy: 0.9868 - val_loss: 0.0509 - val_accuracy: 0.9847\n",
      "Epoch 3/5\n",
      "540/540 - 4s - loss: 0.0327 - accuracy: 0.9898 - val_loss: 0.0484 - val_accuracy: 0.9852\n",
      "Epoch 4/5\n",
      "540/540 - 4s - loss: 0.0287 - accuracy: 0.9909 - val_loss: 0.0392 - val_accuracy: 0.9888\n",
      "Epoch 5/5\n",
      "540/540 - 5s - loss: 0.0240 - accuracy: 0.9922 - val_loss: 0.0361 - val_accuracy: 0.9903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15d4c8ab048>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "model.fit(train_data,\n",
    "          epochs = EPOCHS,\n",
    "          validation_data = (validation_inputs,validation_targets),\n",
    "          validation_steps= 10,\n",
    "          verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] 0.0849 - accuracy: 0.97 - 1s 1s/step - loss: 0.0849 - accuracy: 0.9771\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.08. Test accuracy: 97.71%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
